<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Yzsj Site</title><link>https://www.yizhishuijiao.com/</link><description>Recent content on Yzsj Site</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 03 Dec 2021 18:18:44 +0800</lastBuildDate><atom:link href="https://www.yizhishuijiao.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Daily Paper 01 - Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning</title><link>https://www.yizhishuijiao.com/posts/my-first-post/</link><pubDate>Fri, 03 Dec 2021 18:18:44 +0800</pubDate><guid>https://www.yizhishuijiao.com/posts/my-first-post/</guid><description>今天看的这篇比较简单，是一篇来自 CVPR 2021 的小样本图像分类论文。论文的标题是 Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning1，官方代码2，Fork了一份防删3（看 Issue 的讨论4，这个方法似乎很消耗显存，有时间试试好了）。
基本思想 这篇论文的工作在分类上属于最近比较常见的类别，即学习一个 better embedding。提出的方法也非常简单，一句话来概括就是用「多任务学习」的方式，把自监督学习中的 「学习不变性 Invariant」 和 「学习变性 Equivariant」 两个 「流派」 的损失函数整合起来，再加上标准的分类损失函数，一同学习一个厉害的特征提取器，来提升小样本任务的分类精度。另外作者还用到了一个叫做「多头自蒸馏」的方法进一步涨点，还挺神奇的。不过之前没接触过知识蒸馏，也不知道这个方法是作者发明的还是前人工作。
一句题外话，把不变性和变性损失用多任务学习整合这个想法我在前段时间看自监督学习论文的时候也想到了，不过一直还没有尝试&amp;hellip; 唉，日常被自己菜哭😢。
方法 让我们跳过无聊的「编故事」环节，来看看本文提出的方法吧~
下图就是本文方法的网络架构图。其中这种对称的结构就是所谓的教师-学生结构，右边的是教师网络（也就是旧模型），左边是学生网络（新模型），学生网络学习教师网络（这部分具体如何操作的我也不太理解，之后看完代码再回来补充吧~）。
因为教师和学生网络结构是一样的，所以只关注左边就好了。从图上可以看到，最左侧有 $M$ 簇图片，分别代表了不同的几何变换（Geometric Transformation），也就是说，一组样本会用 $M$ 种变换方式生成 $M$ 组图像，然后这些图像之后会输入同一个特征提取器里提取特征。那这些变换具体是什么呢？原文中举了一些例子，包括：Euclidean transformation、Similarity transformation、Affine transformation 以及 Projective transformation。变换组合的选取与具体的任务和数据集有关，至于本文的具体选取可以参考论文的第四章。
总之，这些样本会同时输入特征提取器网络 $f_{\Theta}$ 来提取特征，这些特征随后被输入三个不同的 MLP 头：$f_{\Psi}$、$f_{\Phi}$和$f_{\Omega}$，之后会解释每个头的作用。特征提取完毕后，网络会用四种不同的方式来处理这些 embedding ，分别是：有监督分类、变性(Equivariant)自监督、不变(Invariant)自监督和多头自蒸馏。
有监督分类
相关的 MLP 头是$f_{\Phi}$。这个头把特征映射和 Softmax 为预测概率，然后计算交叉熵分类损失，文中称之为 $\mathcal{L}_{\text {baseline }}$。
变性(Equivariant)自监督
相关的 MLP 头是$f_{\Psi}$。所谓变性(Equivariant)自监督，就是意在让模型捕捉图像的变化。具体来说，因为有 $M$ 种变换嘛，于是就用 one-hot 的方式给每个样本指定一个标签 $\mathbf{u}$，其中 $\mathbf{u} \in{0,1}^{M}$, $ \sum_{i} \mathbf{u}_{i}=1$。图像特征输入 $f_{\Psi}$，这个头把特征映射为 $M$ 维预测概率，随后利用自动生成的自监督标签计算交叉熵分类损失，文中称这个损失为$\mathcal{L}_{\text {eq }}$。</description></item></channel></rss>