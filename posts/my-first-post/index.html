<!doctype html><html lang=en><head><meta charset=utf-8><meta name=generator content="Hugo 0.85.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="yzsj98"><meta property="og:url" content="https://www.yizhishuijiao.com/posts/my-first-post/"><link rel=canonical href=https://www.yizhishuijiao.com/posts/my-first-post/><link rel=alternate type=application/atom+xml href=https://www.yizhishuijiao.comindex.xml title="Yzsj Site"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/www.yizhishuijiao.com"},"articleSection":"posts","name":"Daily Paper 01 - Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning","headline":"Daily Paper 01 - Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning","description":"今天看的这篇比较简单，是一篇来自 CVPR 2021 的小样本图像分类论文。论文的标题是 Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning1，官方代码2，Fork了一份防删3（看 Issue 的讨论4，这个方法似乎很消耗显存，有时间试试好了）。\n基本思想 这篇论文的工作在分类上属于最近比较常见的类别，即学习一个 better embedding。提出的方法也非常简单，一句话来概括就是用「多任务学习」的方式，把自监督学习中的 「学习不变性 Invariant」 和 「学习变性 Equivariant」 两个 「流派」 的损失函数整合起来，再加上标准的分类损失函数，一同学习一个厉害的特征提取器，来提升小样本任务的分类精度。另外作者还用到了一个叫做「多头自蒸馏」的方法进一步涨点，还挺神奇的。不过之前没接触过知识蒸馏，也不知道这个方法是作者发明的还是前人工作。\n一句题外话，把不变性和变性损失用多任务学习整合这个想法我在前段时间看自监督学习论文的时候也想到了，不过一直还没有尝试\u0026hellip; 唉，日常被自己菜哭😢。\n方法 让我们跳过无聊的「编故事」环节，来看看本文提出的方法吧~\n下图就是本文方法的网络架构图。其中这种对称的结构就是所谓的教师-学生结构，右边的是教师网络（也就是旧模型），左边是学生网络（新模型），学生网络学习教师网络（这部分具体如何操作的我也不太理解，之后看完代码再回来补充吧~）。\n因为教师和学生网络结构是一样的，所以只关注左边就好了。从图上可以看到，最左侧有 $M$ 簇图片，分别代表了不同的几何变换（Geometric Transformation），也就是说，一组样本会用 $M$ 种变换方式生成 $M$ 组图像，然后这些图像之后会输入同一个特征提取器里提取特征。那这些变换具体是什么呢？原文中举了一些例子，包括：Euclidean transformation、Similarity transformation、Affine transformation 以及 Projective transformation。变换组合的选取与具体的任务和数据集有关，至于本文的具体选取可以参考论文的第四章。\n总之，这些样本会同时输入特征提取器网络 $f_{\\Theta}$ 来提取特征，这些特征随后被输入三个不同的 MLP 头：$f_{\\Psi}$、$f_{\\Phi}$和$f_{\\Omega}$，之后会解释每个头的作用。特征提取完毕后，网络会用四种不同的方式来处理这些 embedding ，分别是：有监督分类、变性(Equivariant)自监督、不变(Invariant)自监督和多头自蒸馏。\n  有监督分类\n相关的 MLP 头是$f_{\\Phi}$。这个头把特征映射和 Softmax 为预测概率，然后计算交叉熵分类损失，文中称之为 $\\mathcal{L}_{\\text {baseline }}$。\n  变性(Equivariant)自监督\n相关的 MLP 头是$f_{\\Psi}$。所谓变性(Equivariant)自监督，就是意在让模型捕捉图像的变化。具体来说，因为有 $M$ 种变换嘛，于是就用 one-hot 的方式给每个样本指定一个标签 $\\mathbf{u}$，其中 $\\mathbf{u} \\in{0,1}^{M}$, $ \\sum_{i} \\mathbf{u}_{i}=1$。图像特征输入 $f_{\\Psi}$，这个头把特征映射为 $M$ 维预测概率，随后利用自动生成的自监督标签计算交叉熵分类损失，文中称这个损失为$\\mathcal{L}_{\\text {eq }}$。","inLanguage":"en-US","author":"yzsj98","creator":"yzsj98","publisher":"yzsj98","accountablePerson":"yzsj98","copyrightHolder":"yzsj98","copyrightYear":"2021","datePublished":"2021-12-03 18:18:44 \u002b0800 \u002b0800","dateModified":"2021-12-03 18:18:44 \u002b0800 \u002b0800","url":"https:\/\/www.yizhishuijiao.com\/posts\/my-first-post\/","keywords":["小样本"]}</script><title>Daily Paper 01 - Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning</title><meta property="og:title" content="Daily Paper 01 - Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning"><meta property="og:type" content="article"><meta property="og:description" content="今天看的这篇比较简单，是一篇来自 CVPR 2021 的小样本图像分类论文。论文的标题是 Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning1，官方代码2，Fork了一份防删3（看 Issue 的讨论4，这个方法似乎很消耗显存，有时间试试好了）。
基本思想 这篇论文的工作在分类上属于最近比较常见的类别，即学习一个 better embedding。提出的方法也非常简单，一句话来概括就是用「多任务学习」的方式，把自监督学习中的 「学习不变性 Invariant」 和 「学习变性 Equivariant」 两个 「流派」 的损失函数整合起来，再加上标准的分类损失函数，一同学习一个厉害的特征提取器，来提升小样本任务的分类精度。另外作者还用到了一个叫做「多头自蒸馏」的方法进一步涨点，还挺神奇的。不过之前没接触过知识蒸馏，也不知道这个方法是作者发明的还是前人工作。
一句题外话，把不变性和变性损失用多任务学习整合这个想法我在前段时间看自监督学习论文的时候也想到了，不过一直还没有尝试&amp;hellip; 唉，日常被自己菜哭😢。
方法 让我们跳过无聊的「编故事」环节，来看看本文提出的方法吧~
下图就是本文方法的网络架构图。其中这种对称的结构就是所谓的教师-学生结构，右边的是教师网络（也就是旧模型），左边是学生网络（新模型），学生网络学习教师网络（这部分具体如何操作的我也不太理解，之后看完代码再回来补充吧~）。
因为教师和学生网络结构是一样的，所以只关注左边就好了。从图上可以看到，最左侧有 $M$ 簇图片，分别代表了不同的几何变换（Geometric Transformation），也就是说，一组样本会用 $M$ 种变换方式生成 $M$ 组图像，然后这些图像之后会输入同一个特征提取器里提取特征。那这些变换具体是什么呢？原文中举了一些例子，包括：Euclidean transformation、Similarity transformation、Affine transformation 以及 Projective transformation。变换组合的选取与具体的任务和数据集有关，至于本文的具体选取可以参考论文的第四章。
总之，这些样本会同时输入特征提取器网络 $f_{\Theta}$ 来提取特征，这些特征随后被输入三个不同的 MLP 头：$f_{\Psi}$、$f_{\Phi}$和$f_{\Omega}$，之后会解释每个头的作用。特征提取完毕后，网络会用四种不同的方式来处理这些 embedding ，分别是：有监督分类、变性(Equivariant)自监督、不变(Invariant)自监督和多头自蒸馏。
  有监督分类
相关的 MLP 头是$f_{\Phi}$。这个头把特征映射和 Softmax 为预测概率，然后计算交叉熵分类损失，文中称之为 $\mathcal{L}_{\text {baseline }}$。
  变性(Equivariant)自监督
相关的 MLP 头是$f_{\Psi}$。所谓变性(Equivariant)自监督，就是意在让模型捕捉图像的变化。具体来说，因为有 $M$ 种变换嘛，于是就用 one-hot 的方式给每个样本指定一个标签 $\mathbf{u}$，其中 $\mathbf{u} \in{0,1}^{M}$, $ \sum_{i} \mathbf{u}_{i}=1$。图像特征输入 $f_{\Psi}$，这个头把特征映射为 $M$ 维预测概率，随后利用自动生成的自监督标签计算交叉熵分类损失，文中称这个损失为$\mathcal{L}_{\text {eq }}$。"><meta name=description content="今天看的这篇比较简单，是一篇来自 CVPR 2021 的小样本图像分类论文。论文的标题是 Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning1，官方代码2，Fork了一份防删3（看 Issue 的讨论4，这个方法似乎很消耗显存，有时间试试好了）。
基本思想 这篇论文的工作在分类上属于最近比较常见的类别，即学习一个 better embedding。提出的方法也非常简单，一句话来概括就是用「多任务学习」的方式，把自监督学习中的 「学习不变性 Invariant」 和 「学习变性 Equivariant」 两个 「流派」 的损失函数整合起来，再加上标准的分类损失函数，一同学习一个厉害的特征提取器，来提升小样本任务的分类精度。另外作者还用到了一个叫做「多头自蒸馏」的方法进一步涨点，还挺神奇的。不过之前没接触过知识蒸馏，也不知道这个方法是作者发明的还是前人工作。
一句题外话，把不变性和变性损失用多任务学习整合这个想法我在前段时间看自监督学习论文的时候也想到了，不过一直还没有尝试&amp;hellip; 唉，日常被自己菜哭😢。
方法 让我们跳过无聊的「编故事」环节，来看看本文提出的方法吧~
下图就是本文方法的网络架构图。其中这种对称的结构就是所谓的教师-学生结构，右边的是教师网络（也就是旧模型），左边是学生网络（新模型），学生网络学习教师网络（这部分具体如何操作的我也不太理解，之后看完代码再回来补充吧~）。
因为教师和学生网络结构是一样的，所以只关注左边就好了。从图上可以看到，最左侧有 $M$ 簇图片，分别代表了不同的几何变换（Geometric Transformation），也就是说，一组样本会用 $M$ 种变换方式生成 $M$ 组图像，然后这些图像之后会输入同一个特征提取器里提取特征。那这些变换具体是什么呢？原文中举了一些例子，包括：Euclidean transformation、Similarity transformation、Affine transformation 以及 Projective transformation。变换组合的选取与具体的任务和数据集有关，至于本文的具体选取可以参考论文的第四章。
总之，这些样本会同时输入特征提取器网络 $f_{\Theta}$ 来提取特征，这些特征随后被输入三个不同的 MLP 头：$f_{\Psi}$、$f_{\Phi}$和$f_{\Omega}$，之后会解释每个头的作用。特征提取完毕后，网络会用四种不同的方式来处理这些 embedding ，分别是：有监督分类、变性(Equivariant)自监督、不变(Invariant)自监督和多头自蒸馏。
  有监督分类
相关的 MLP 头是$f_{\Phi}$。这个头把特征映射和 Softmax 为预测概率，然后计算交叉熵分类损失，文中称之为 $\mathcal{L}_{\text {baseline }}$。
  变性(Equivariant)自监督
相关的 MLP 头是$f_{\Psi}$。所谓变性(Equivariant)自监督，就是意在让模型捕捉图像的变化。具体来说，因为有 $M$ 种变换嘛，于是就用 one-hot 的方式给每个样本指定一个标签 $\mathbf{u}$，其中 $\mathbf{u} \in{0,1}^{M}$, $ \sum_{i} \mathbf{u}_{i}=1$。图像特征输入 $f_{\Psi}$，这个头把特征映射为 $M$ 维预测概率，随后利用自动生成的自监督标签计算交叉熵分类损失，文中称这个损失为$\mathcal{L}_{\text {eq }}$。"><meta property="og:locale" content="en-us"><style>h1,h2,h3,h4,h5,h6{border-bottom:1px solid;border-color:#eee}html{scroll-behavior:smooth}body{font-family:bree serif,noto serif sc;-webkit-font-smoothing:antialiased;margin:0 20px}code{font-family:fira code}article{max-width:800px;margin-left:auto;margin-right:auto}a{color:#000;text-decoration:none}a:hover{font-weight:600;text-decoration:underline}.post-ads{margin:50px 0}.markdown-body{font-size:18px;max-width:100%}.markdown-body a{text-decoration:underline;text-decoration-color:#000}.markdown-body pre{padding:16px;overflow:auto;border-radius:10px}.markdown-body code{padding:.2em .4em;font-size:85%;background-color:#f6f8fa;border-radius:6px}.markdown-body pre>code{padding:0;font-size:100%;background-color:inherit;border:0}.Chinese .markdown-body{line-height:200%}.site-date-catalog{font-size:2rem}.header-title{font-size:2rem;font-weight:700;margin-top:32px;font-family:bungee shade,sans-serif}.header-title a{text-decoration:none}.header-subtitle{color:#666}.header-items{margin:10px 0}.header-item{margin:0 5px}.header-line{width:100%;border-bottom:1px solid;border-color:#7c7c7c}.lang-switch{font-weight:600}#posts-list{min-height:600px}.posts-line{font-size:1.2rem;margin:12px 0}.posts-categories{font-size:.8rem;margin:auto;text-align:center}.posts-category{padding:3px 0;border:#000 2px solid;border-radius:5px}.site-footer{margin-top:50px}.site-footer-item{margin-right:12px}.post-content img{max-width:100%;display:block;margin-right:auto;margin-top:12px}.post-header{margin-bottom:15px}.post-title{margin:10px 0;font-size:2rem;font-weight:600}.post-tags{display:inline;font-weight:600;padding:2px 5px;margin-right:6px;border:#000 2px solid;border-radius:5px}.post-date{color:#999}.post-author{float:right;font-weight:600}.page-content{min-height:60%}.post-content{margin-bottom:50px}.post-content p{hyphens:auto;line-height:1.8;text-justify:ideographic;margin-bottom:1em}.related-content{border-width:3px;border-style:solid;border-color:#000;padding:0 10px;margin-bottom:50px;margin-top:100px}.related-content li{margin:5px 0}.contents{background-color:rgba(233,233,233,.5);width:35%;position:relative;border-width:3px;border-style:double;border-radius:4px;border-color:#000;padding:3px 5px;margin-bottom:10px;margin-top:0}.contents ol{padding-right:30px}.contents ol ol{padding-right:10px}#content-body{display:block}.contents .hide-tool{position:absolute;right:5px;top:0}.contents .content-title{font-size:1.25em}.taxonomy-term{font-size:3rem}.gallery-img{text-align:center}.gallery-img span{text-align:center}.gallery-img-desc{font-size:.8em;font-weight:800}#disqus_thread{position:relative}#disqus_thread:after{content:"";display:block;height:55px;width:100%;position:absolute;bottom:0;background:#fff}@media screen and (max-width:600px){.header-title,.header-subtitle,.header-items{text-align:center}.posts-line{font-size:16px}.markdown-body{font-size:16px}.post-title{font-size:2rem}.post-content p{letter-spacing:.05em}}@media screen and (max-width:48em){.posts-category{display:none}.contents .hide-tool{display:none}.contents{width:100%}}</style><style>.container,.container-fluid{margin-right:auto;margin-left:auto}.container-fluid{padding-right:2rem;padding-left:2rem}.row{box-sizing:border-box;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 1 auto;flex:initial;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-.5rem;margin-left:-.5rem}.row.reverse{-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse}.col.reverse{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse}.col-xs,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9,.col-xs-offset-0,.col-xs-offset-1,.col-xs-offset-10,.col-xs-offset-11,.col-xs-offset-12,.col-xs-offset-2,.col-xs-offset-3,.col-xs-offset-4,.col-xs-offset-5,.col-xs-offset-6,.col-xs-offset-7,.col-xs-offset-8,.col-xs-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-xs{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-xs-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-xs-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-xs-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-xs-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-xs-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-xs-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-xs-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-xs-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-xs-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-xs-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-xs-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-xs-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-xs-offset-0{margin-left:0}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-11{margin-left:91.66666667%}.start-xs{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-xs{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-xs{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-xs{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-xs{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-xs{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-xs{-ms-flex-pack:distribute;justify-content:space-around}.between-xs{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-xs{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-xs{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}@media only screen and (min-width:48em){.container{width:49rem}.col-sm,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-sm-offset-0,.col-sm-offset-1,.col-sm-offset-10,.col-sm-offset-11,.col-sm-offset-12,.col-sm-offset-2,.col-sm-offset-3,.col-sm-offset-4,.col-sm-offset-5,.col-sm-offset-6,.col-sm-offset-7,.col-sm-offset-8,.col-sm-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-sm{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-sm-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-sm-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-sm-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-sm-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-sm-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-sm-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-sm-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-sm-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-sm-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-sm-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-sm-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-sm-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-sm-offset-0{margin-left:0}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-11{margin-left:91.66666667%}.start-sm{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-sm{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-sm{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-sm{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-sm{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-sm{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-sm{-ms-flex-pack:distribute;justify-content:space-around}.between-sm{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-sm{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-sm{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:64em){.container{width:65rem}.col-md,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-md-offset-0,.col-md-offset-1,.col-md-offset-10,.col-md-offset-11,.col-md-offset-12,.col-md-offset-2,.col-md-offset-3,.col-md-offset-4,.col-md-offset-5,.col-md-offset-6,.col-md-offset-7,.col-md-offset-8,.col-md-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-md{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-md-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-md-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-md-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-md-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-md-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-md-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-md-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-md-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-md-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-md-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-md-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-md-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-md-offset-0{margin-left:0}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-3{margin-left:25%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-6{margin-left:50%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-9{margin-left:75%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-11{margin-left:91.66666667%}.start-md{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-md{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-md{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-md{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-md{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-md{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-md{-ms-flex-pack:distribute;justify-content:space-around}.between-md{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-md{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-md{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:75em){.container{width:76rem}.col-lg,.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-lg-offset-0,.col-lg-offset-1,.col-lg-offset-10,.col-lg-offset-11,.col-lg-offset-12,.col-lg-offset-2,.col-lg-offset-3,.col-lg-offset-4,.col-lg-offset-5,.col-lg-offset-6,.col-lg-offset-7,.col-lg-offset-8,.col-lg-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-lg{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-lg-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-lg-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-lg-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-lg-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-lg-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-lg-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-lg-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-lg-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-lg-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-lg-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-lg-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-lg-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-lg-offset-0{margin-left:0}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-11{margin-left:91.66666667%}.start-lg{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-lg{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-lg{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-lg{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-lg{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-lg{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-lg{-ms-flex-pack:distribute;justify-content:space-around}.between-lg{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-lg{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-lg{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}</style><link href=/index.xml rel=alternate type=application/rss+xml title="Yzsj Site"><link href="https://fonts.loli.net/css?family=Bree+Serif|Bungee+Shade|Fira+Code|Roboto+Slab:400,600|Noto+Serif+SC:600&display=swap" rel=stylesheet><script src=/js/tools.js></script></head><body><article class=post id=article><div class=row><div class=col-xs-12><div class=site-header><header><div class=header-title><a href=/>YZSJ Site</a></div><div class=header-subtitle>Hello, I am yzsj98</div></header><div class="row end-md center-xs header-items"><div class=header-item><a href=https://github.com/yzsj98 target=_blank>Github</a></div><div class=header-item><a href=https://yizhishuijiao.wordpress.com target=_blank>草稿</a></div><div class=header-item><a href=/index.xml target=_blank>RSS</a></div></div><div class="row end-xs"></div><div class=header-line></div></div><header class=post-header><div class=post-title>Daily Paper 01 - Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning</div><time class=post-date datetime="2021-12-03 18:18:44 +0800">03 Dec 2021</time>
<span class=post-date>/ @yzsj98</span></header><div class="post-content markdown-body"><div class=contents><span class=content-title>Contents</span>
<a class=hide-tool href=# onclick=toggleContents() class=hide>[Toggle]</a><div id=content-body><nav id=TableOfContents><ol><li><a href=#基本思想>基本思想</a></li><li><a href=#方法>方法</a></li><li><a href=#实验结果>实验结果</a></li></ol></nav></div></div><p>今天看的这篇比较简单，是一篇来自 CVPR 2021 的小样本图像分类论文。论文的标题是 Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，官方代码<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>，Fork了一份防删<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>（看 Issue 的讨论<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>，这个方法似乎很消耗显存，有时间试试好了）。</p><h2 id=基本思想>基本思想</h2><p>这篇论文的工作在分类上属于最近比较常见的类别，即学习一个 better embedding。提出的方法也非常简单，一句话来概括就是用「多任务学习」的方式，把自监督学习中的 「学习不变性 Invariant」 和 「学习变性 Equivariant」 两个 「流派」 的损失函数整合起来，再加上标准的分类损失函数，一同学习一个厉害的特征提取器，来提升小样本任务的分类精度。另外作者还用到了一个叫做「多头自蒸馏」的方法进一步涨点，还挺神奇的。不过之前没接触过知识蒸馏，也不知道这个方法是作者发明的还是前人工作。</p><p>一句题外话，把不变性和变性损失用多任务学习整合这个想法我在前段时间看自监督学习论文的时候也想到了，不过一直还没有尝试&mldr; 唉，日常被自己菜哭😢。</p><h2 id=方法>方法</h2><p>让我们跳过无聊的「编故事」环节，来看看本文提出的方法吧~</p><p>下图就是本文方法的网络架构图。其中这种对称的结构就是所谓的<strong>教师-学生结构</strong>，右边的是教师网络（也就是旧模型），左边是学生网络（新模型），学生网络学习教师网络（这部分具体如何操作的我也不太理解，之后看完代码再回来补充吧~）。</p><p><img src=assets/20210609101231_4ce61373b0302993ab8a73e8b7e6f7dd.png alt></p><p>因为教师和学生网络结构是一样的，所以只关注左边就好了。从图上可以看到，最左侧有 $M$ 簇图片，分别代表了不同的几何变换（Geometric Transformation），也就是说，一组样本会用 $M$ 种变换方式生成 $M$ 组图像，然后这些图像之后会输入同一个特征提取器里提取特征。那这些变换具体是什么呢？原文中举了一些例子，包括：Euclidean transformation、Similarity transformation、Affine transformation 以及 Projective transformation。变换组合的选取与具体的任务和数据集有关，至于本文的具体选取可以参考论文的第四章。</p><p>总之，这些样本会同时输入特征提取器网络 $f_{\Theta}$ 来提取特征，这些特征随后被输入三个不同的 MLP 头：$f_{\Psi}$、$f_{\Phi}$和$f_{\Omega}$，之后会解释每个头的作用。特征提取完毕后，网络会用四种不同的方式来处理这些 embedding ，分别是：有监督分类、变性(Equivariant)自监督、不变(Invariant)自监督和多头自蒸馏。</p><ol><li><p><strong>有监督分类</strong></p><p>相关的 MLP 头是$f_{\Phi}$。这个头把特征映射和 Softmax 为预测概率，然后计算交叉熵分类损失，文中称之为 $\mathcal{L}_{\text {baseline }}$。</p></li><li><p><strong>变性(Equivariant)自监督</strong></p><p>相关的 MLP 头是$f_{\Psi}$。所谓变性(Equivariant)自监督，就是意在让模型捕捉图像的变化。具体来说，因为有 $M$ 种变换嘛，于是就用 one-hot 的方式给每个样本指定一个标签 $\mathbf{u}$，其中 $\mathbf{u} \in{0,1}^{M}$, $ \sum_{i} \mathbf{u}_{i}=1$。图像特征输入 $f_{\Psi}$，这个头把特征映射为 $M$ 维预测概率，随后利用自动生成的自监督标签计算交叉熵分类损失，文中称这个损失为$\mathcal{L}_{\text {eq }}$。</p></li><li><p><strong>不变(Invariant)自监督</strong></p><p>相关的 MLP 头是$f_{\Omega}$。所谓不变(Invariant)自监督，就是让模型捕捉图像的变化中的不变性。作者在这里巧妙的使用了自监督学习中的对比学习(Contrastive learning)方法来实现。$f_{\Omega}$头类似于对比学习中常用的投影头，主要是对特征进行降维。</p><p>如架构图所示，类似于MoCo<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>，作者用了一个 Memory Bank 来将负样本数量与批大小解耦，这部分的损失函数如下：</p><div>$$
\mathcal{L}_{i n}=-\frac{1}{M} \sum_{m=0}^{M-1} \log \left(h\left(\mathbf{v}^{r}, \mathbf{v}^{m}\right)\right)\left\{\begin{array}{l}
m \neq 0 \rightarrow \mathbf{v}^{r}=\mathbf{v}^{0} \\
m=0 \rightarrow \mathbf{v}^{r}=\tilde{\mathbf{v}}^{0}
\end{array}\right.
$$</div><p>其中，$\mathbf{v}^{0}$ 表示没有变换的原始图像的特征，$\tilde{\mathbf{v}}^{0}$ 表示 Memory Bank 中 $\mathbf{v}^{0}$ 的旧的值。$h(\cdot)$ 定义为：</p><p>$$
h\left(\mathbf{v}^{r}, \mathbf{v}^{m}\right)=\frac{\exp \left(\frac{s\left(\mathbf{v}^{r}, \mathbf{v}^{m}\right)}{\tau}\right)}{\exp \left(\frac{s\left(\mathbf{v}^{r}, \mathbf{v}^{m}\right)}{\tau}\right)+\sum_{\mathbf{v}^{\prime} \in \mathcal{D}_{n}} \exp \left(\frac{s\left(\mathbf{v}^{\prime}, \mathbf{v}^{m}\right)}{\tau}\right)}
$$</p><p>这个就是常规的 Contrastive Loss，注意作者还最大化了$\mathbf{v}^{0}$ 和 $\tilde{\mathbf{v}}^{0}$ 的相似度，解释说这样能够使得训练稳定。</p></li><li><p><strong>多头自蒸馏</strong></p><p>这部分没有新的MLP头，损失定义如下：</p><div>$$
\begin{aligned}
\mathcal{L}_{k d}=& \mathrm{KL}\left(f_{\Theta, \Phi}^{t}(\mathrm{x}), f_{\Theta, \Phi}(\mathrm{x})\right)+\mathrm{KL}\left(f_{\Theta, \Psi}^{t}(\mathrm{x}), f_{\Theta, \Psi}(\mathrm{x})\right) \\
&+\mathcal{L}_{2}\left(f_{\Theta, \Omega}^{t}(\mathrm{x}), f_{\Theta, \Omega}(\mathrm{x})\right)
\end{aligned}
$$</div><p>其中 $f_{(\cdot)}^{t}$ 和$f_{(\cdot)}$ 分别表示教师网络和学生网络，损失定义为他们输出的 KL 散度和，目的就是让学生网络学习教师的输出。注意这里由于不变(Invariant)自监督中的$f_{\Omega}$头输出的不是 softmax 概率，所以作者改成了 L2 损失（原因暂时不清楚，可能是工程经验或者是 KL散度的性质决定的）。</p></li></ol><p>整体的损失就是各部分损失加和。</p><h2 id=实验结果>实验结果</h2><p>直接摆出 mini-IN 和 tiered-IN 的结果，在 inductive FSL 里面应该是 SOTA 吧。另外消融实验里面有一些有意思的实验，比如探讨不同的自监督方法的效果，有时间可以细看一下。</p><p><strong>mini-IN</strong>：</p><p><img src=assets/20210609113937_df5706835dd8a5126dd5fc29c0330a9a.png alt></p><p><strong>tiered-IN</strong>：</p><p><img src=assets/20210609113956_ff6ec25a24f89969f992d2e2d22ef091.png alt></p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>M. N. Rizve, S. Khan, F. S. Khan, and M. Shah, “Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning,” <em>arXiv:2103.01315 [cs]</em>, Apr. 2021, Accessed: Jun. 02, 2021. [Online]. <a href=http://arxiv.org/abs/2103.01315>Available</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href=https://github.com/nayeemrizve/invariance-equivariance>nayeemrizve/invariance-equivariance</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p><a href=https://github.com/TildenJ/invariance-equivariance>TildenJ/invariance-equivariance</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p><a href=https://github.com/nayeemrizve/invariance-equivariance/issues/2>Pretrained models ## 2</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum Contrast for Unsupervised Visual Representation Learning,” <em>arXiv:1911.05722 [cs]</em>, Mar. 2020, Accessed: Jan. 06, 2021. [Online]. <a href=http://arxiv.org/abs/1911.05722>Available</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><div class="row middle-xs"><div class=col-xs-12><div class=post-tags><a href=/tags/%E5%B0%8F%E6%A0%B7%E6%9C%AC/>小样本</a></div></div></div><div class=row><div class=col-xs-12></div></div><div style=height:50px></div><div class=site-footer></div></div></div></article><script>MathJax={tex:{inlineMath:[["$","$"]]},displayMath:[["$$","$$"],["[[","]]"]],svg:{fontCache:"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></script><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js></script><script>document.querySelectorAll('.post-content img').forEach(function(a){a.parentNode.nodeName.toLowerCase()==='a'||mediumZoom(a,{margin:24})})</script><script></script></body></html>